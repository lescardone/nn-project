{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_transfer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNalF/9qwrMvmz0eaYkQfE7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lescardone/deep-learning-project/blob/main/CNN_transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gE3lQms2cBhE",
        "outputId": "8229c682-1c1b-4df8-f932-225f72c5e152"
      },
      "source": [
        "import os, shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/grive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/grive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVETGKwDcoGX"
      },
      "source": [
        "# Directories for our training,\n",
        "# validation and test splits\n",
        "base_dir = '/content/grive/My Drive/Metis/Deep-Learning/deep-learning-project/photos/small'\n",
        "\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "test_dir = os.path.join(base_dir, 'test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEwynmK7cWMD",
        "outputId": "17f712f8-80fa-4dee-9190-3d257b8ea0ba"
      },
      "source": [
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # This is the target directory\n",
        "        train_dir,\n",
        "        # All images will be resized to 150x150\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1000 images belonging to 2 classes.\n",
            "Found 600 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5Rqz-EHeNsG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c7991f5-077e-444f-f648-acf2a48160bd"
      },
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "conv_base = VGG16(weights='imagenet',\n",
        "                  include_top=False,\n",
        "                  input_shape=(150, 150, 3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "off4quniQst7"
      },
      "source": [
        "from tensorflow.keras.applications import MobileNet\n",
        "conv_mobile = MobileNet(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(224,224,3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--Fbs9o2RgXl"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "base_dir = '/content/grive/My Drive/Metis/Deep-Learning/deep-learning-project/photos/small'\n",
        "\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "batch_size = 20\n",
        "\n",
        "def extract_features(directory, sample_count):\n",
        "    features = np.zeros(shape=(sample_count, 7, 7, 1024))\n",
        "    labels = np.zeros(shape=(sample_count))\n",
        "    generator = datagen.flow_from_directory(\n",
        "        directory,\n",
        "        target_size=(224, 224),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')\n",
        "    i = 0\n",
        "    for inputs_batch, labels_batch in generator:\n",
        "        features_batch = conv_mobile.predict(inputs_batch)\n",
        "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
        "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
        "        i += 1\n",
        "        if i * batch_size >= sample_count:\n",
        "            # Note that since generators yield data indefinitely in a loop,\n",
        "            # we must `break` after every image has been seen once.\n",
        "            break\n",
        "    return features, labels, generator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pk7SfE9sR3k6",
        "outputId": "2addf571-e683-4162-f715-7ff8f087d2b2"
      },
      "source": [
        "train_features_m, train_labels_m, train_generator_m = extract_features(train_dir, 1000)\n",
        "validation_features_m, validation_labels_m, validation_generator_m = extract_features(validation_dir, 600)\n",
        "test_features_m, test_labels_m, test_generator_m = extract_features(test_dir, 652)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1000 images belonging to 2 classes.\n",
            "Found 600 images belonging to 2 classes.\n",
            "Found 652 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsDJG56mUILF"
      },
      "source": [
        "train_features_m = np.reshape(train_features_m, (1000, 7 * 7 * 1024))\n",
        "validation_features_m = np.reshape(validation_features_m, (600, 7 * 7 * 1024))\n",
        "test_features_m = np.reshape(test_features_m, (652, 7 * 7 * 1024))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY4LY2jGTJx6"
      },
      "source": [
        "from keras.layers import MaxPooling2D, Conv2D, Flatten, Dense, Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import RMSprop, Adam\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH5iuvVmaelc"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(256, activation='relu', input_dim=(7 * 7 * 1024)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=RMSprop(learning_rate=2e-5),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model.fit(train_features_m, train_labels_m,\n",
        "                    epochs=50,\n",
        "                    batch_size=20,\n",
        "                    validation_data=(validation_features_m, validation_labels_m))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6dGYV0_YWyC",
        "outputId": "cfe813d1-e640-41ae-b4fb-b94fb0b807ea"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(256, activation='relu', input_dim=(7 * 7 * 1024)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128, activation='relu', input_dim=(7 * 7 * 1024)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu', input_dim=(7 * 7 * 1024)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=2e-5),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model.fit(train_features_m, train_labels_m,\n",
        "                    epochs=20,\n",
        "                    batch_size=20,\n",
        "                    validation_data=(validation_features_m, validation_labels_m))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "50/50 [==============================] - 5s 80ms/step - loss: 1.4960 - acc: 0.4753 - val_loss: 0.6767 - val_acc: 0.5700\n",
            "Epoch 2/20\n",
            "50/50 [==============================] - 4s 73ms/step - loss: 0.9763 - acc: 0.5626 - val_loss: 0.6598 - val_acc: 0.5967\n",
            "Epoch 3/20\n",
            "50/50 [==============================] - 4s 73ms/step - loss: 0.8556 - acc: 0.5410 - val_loss: 0.6357 - val_acc: 0.6383\n",
            "Epoch 4/20\n",
            "50/50 [==============================] - 4s 72ms/step - loss: 0.8170 - acc: 0.5399 - val_loss: 0.6190 - val_acc: 0.6983\n",
            "Epoch 5/20\n",
            "50/50 [==============================] - 4s 73ms/step - loss: 0.7606 - acc: 0.5812 - val_loss: 0.6104 - val_acc: 0.6700\n",
            "Epoch 6/20\n",
            "50/50 [==============================] - 4s 73ms/step - loss: 0.6413 - acc: 0.6636 - val_loss: 0.5934 - val_acc: 0.7400\n",
            "Epoch 7/20\n",
            "50/50 [==============================] - 4s 72ms/step - loss: 0.6007 - acc: 0.6806 - val_loss: 0.5804 - val_acc: 0.7200\n",
            "Epoch 8/20\n",
            "50/50 [==============================] - 4s 73ms/step - loss: 0.5935 - acc: 0.7067 - val_loss: 0.5557 - val_acc: 0.7283\n",
            "Epoch 9/20\n",
            "50/50 [==============================] - 4s 72ms/step - loss: 0.5051 - acc: 0.7500 - val_loss: 0.5502 - val_acc: 0.7300\n",
            "Epoch 10/20\n",
            "50/50 [==============================] - 4s 73ms/step - loss: 0.5201 - acc: 0.7452 - val_loss: 0.5484 - val_acc: 0.7300\n",
            "Epoch 11/20\n",
            "50/50 [==============================] - 4s 72ms/step - loss: 0.4633 - acc: 0.7764 - val_loss: 0.5400 - val_acc: 0.7350\n",
            "Epoch 12/20\n",
            "50/50 [==============================] - 4s 73ms/step - loss: 0.4569 - acc: 0.7911 - val_loss: 0.5274 - val_acc: 0.7450\n",
            "Epoch 13/20\n",
            "50/50 [==============================] - 4s 73ms/step - loss: 0.4316 - acc: 0.7861 - val_loss: 0.5247 - val_acc: 0.7367\n",
            "Epoch 14/20\n",
            "50/50 [==============================] - 4s 73ms/step - loss: 0.4075 - acc: 0.8262 - val_loss: 0.5182 - val_acc: 0.7567\n",
            "Epoch 15/20\n",
            "50/50 [==============================] - 4s 74ms/step - loss: 0.3830 - acc: 0.8233 - val_loss: 0.5170 - val_acc: 0.7550\n",
            "Epoch 16/20\n",
            "50/50 [==============================] - 4s 73ms/step - loss: 0.3792 - acc: 0.8110 - val_loss: 0.5388 - val_acc: 0.7250\n",
            "Epoch 17/20\n",
            "50/50 [==============================] - 4s 72ms/step - loss: 0.3492 - acc: 0.8485 - val_loss: 0.5095 - val_acc: 0.7500\n",
            "Epoch 18/20\n",
            "50/50 [==============================] - 4s 73ms/step - loss: 0.2998 - acc: 0.8643 - val_loss: 0.5201 - val_acc: 0.7417\n",
            "Epoch 19/20\n",
            "50/50 [==============================] - 4s 74ms/step - loss: 0.2973 - acc: 0.8727 - val_loss: 0.5400 - val_acc: 0.7450\n",
            "Epoch 20/20\n",
            "50/50 [==============================] - 4s 73ms/step - loss: 0.2851 - acc: 0.8618 - val_loss: 0.5279 - val_acc: 0.7467\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9rdSIpjzklx",
        "outputId": "c9b131f6-38b7-48cf-af48-cc97aa944a01"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "base_dir = '/content/grive/My Drive/Metis/Deep-Learning/deep-learning-project/photos/small'\n",
        "\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "batch_size = 20\n",
        "\n",
        "def extract_features(directory, sample_count):\n",
        "    features = np.zeros(shape=(sample_count, 4, 4, 512))\n",
        "    labels = np.zeros(shape=(sample_count))\n",
        "    generator = datagen.flow_from_directory(\n",
        "        directory,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')\n",
        "    i = 0\n",
        "    for inputs_batch, labels_batch in generator:\n",
        "        features_batch = conv_base.predict(inputs_batch)\n",
        "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
        "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
        "        i += 1\n",
        "        if i * batch_size >= sample_count:\n",
        "            # Note that since generators yield data indefinitely in a loop,\n",
        "            # we must `break` after every image has been seen once.\n",
        "            break\n",
        "    return features, labels, generator\n",
        "\n",
        "train_features, train_labels, train_generator = extract_features(train_dir, 1000)\n",
        "validation_features, validation_labels, validation_generator = extract_features(validation_dir, 600)\n",
        "test_features, test_labels, test_generator = extract_features(test_dir, 652)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1000 images belonging to 2 classes.\n",
            "Found 600 images belonging to 2 classes.\n",
            "Found 652 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82pP_HDpNnMn"
      },
      "source": [
        "train_features = np.reshape(train_features, (1000, 4 * 4 * 512))\n",
        "validation_features = np.reshape(validation_features, (600, 4 * 4 * 512))\n",
        "test_features = np.reshape(test_features, (652, 4 * 4 * 512))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kaa-F5EnOFqL"
      },
      "source": [
        "from keras.layers import MaxPooling2D, Conv2D, Flatten, Dense, Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(256, activation='relu', input_dim=4 * 4 * 512))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=RMSprop(learning_rate=2e-5),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model.fit(train_featbures, train_labels,\n",
        "                    epochs=50,\n",
        "                    batch_size=20,\n",
        "                    validation_data=(validation_features, validation_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhZDQrvBUEht"
      },
      "source": [
        "model.save('happy_and_sad_small_transfer_mobile_adam_1.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}